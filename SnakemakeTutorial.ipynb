{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4257d7-e844-4a0a-8f5d-e17d65791782",
   "metadata": {},
   "source": [
    "# Snakemake Dimuon Tutorial\n",
    "\n",
    "[Snakemake](https://snakemake.readthedocs.io/en/stable/) is a popular *workflow management system* designed to create reproducible and scalable analyses and data processing pipelines.\n",
    "\n",
    "Its name is composed of two words:\n",
    " * **Snake**: because it inherits the language to define the workflows from Python, but extends it to improve the readability of the workflows by minimizing the boilerplates;\n",
    " * **make**: because it takes inspiration from GNU Make to implement a dependency chain to regenerate only those dependencies that actually changed, while letting the cached artifacts untouched. This feature makes it extremely suitable for focussing on long pipelines and developing, testing and tuning multiple times a small step, and then seaminglessly plug into the whole workflow and see the global effect.\n",
    "\n",
    "**Snakemake** is designed to easily scale across distributed resources. Batch systems, grids and clouds have been interfaced with Snakemake to execute the steps of the workflow remotely. This aspect will not be covered in this tutorial, but you may check out [CERN's reana](reana.cern.ch) and AWS [Tibanna](https://tibanna.readthedocs.io/en/latest/) for cloud-native integrations, and [Snakemake profiles](https://github.com/snakemake-profiles/doc) for instructions related to batch systems (such as HTCondor and Slurm).\n",
    "\n",
    "In this tutorial we will focus on the logics to define a workflow and on tips and tricks to employ a variety of other tools, such as bash, python and jupyter, to define the \"jobs\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e4e88-411a-4cc2-acd8-ad77613064e0",
   "metadata": {},
   "source": [
    "## Directed Acyclic Graphs\n",
    "\n",
    "A workflow is represented in Snakemake as a Directed Acyclic Graph (DAG). \n",
    "Most often we will use the word DAG and workflows as synonyms, even though you can thing as DAGs as a generic concept of graph theory describing a particular kind of workflow.\n",
    "\n",
    "But what do we mean by *Directed Acyclic Graphs*? Let's start discussing these three words in reverse order:\n",
    " * **Graph**: In general, a *graph* is a collection of nodes, with edges defining the \"connections\" between nodes, in the context of workflow management,\n",
    "   * *Nodes* represent *tasks* performed in a single *job*;\n",
    "   * *Edges* represent the dependencies of a job on the output of another;\n",
    " * **Acyclic**: a graph is acyclic if there are no loops defined by the edges, in the context of workflow management this means that a dependency loops are not admitted: the job *A* cannot depend on the output of job *B* if *B* depends on the output of job *A*.\n",
    " * **Directed**: in a *directed graph*, edges have a direction. This simply implies that the relation \"depends on\" is not symmetric between the parties.\n",
    "\n",
    "We will use the Python Image Library (PIL) to visualize DAGs througout the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67bc6ec-120c-4d62-abba-c3411ef1c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c132286-03c6-477c-aaf7-f04e6b971c46",
   "metadata": {},
   "source": [
    "## Installing Snakemake in a virtual environment\n",
    "\n",
    "Snakemake can be [installed](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) with several package managers, the recommended ones are mamba and bioconda which are flavors of conda. \n",
    "This tutorial has been designed to run on CERN Swan that does not support (yet?) user's defined kernels. \n",
    "Hence, we will use PyPI and its command-line interface `pip`.\n",
    "\n",
    "To avoid cluttering your global namespace, let's create a virtual environment with `venv` and let's install Snakemake there.\n",
    "Then we will install `snakemake`in the newly created environment and call it with the `-v` flag to print its version in the notebook.\n",
    "\n",
    "Note that here and afterwards, we will invoke snakemake as `<name of the venv env>/bin/snakemake` in other setups you can avoid this by `activate`-ing your environment (`source <name of the venv env>/bin/activate` and then simply `snakemake`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393908d9-f966-460f-b7dd-2562b0619224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Remove the virtual environment if it exists and re-create  it\n",
    "rm -rf env\n",
    "python3 -m venv env\n",
    "\n",
    "# Install snakemake in the new virtual env\n",
    "env/bin/python3 -m  pip install -qU snakemake pulp==2.7.0\n",
    "\n",
    "# Print the version of the installed Snakemake\n",
    "env/bin/snakemake -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028b851-5599-49eb-922f-2dc90f9526a3",
   "metadata": {},
   "source": [
    "## Hello world!\n",
    "\n",
    "Let's start from the simplest possible workflow: a single task, without dependencies, executed once.\n",
    "It's pretty useless, agreed... but it's a way to start discussing the syntax for defining a task, or, in Snakemake jargon, a *rule*.\n",
    "\n",
    "According to the official [documentations](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html), *\"a Snakemake **workflow** defines a data analysis in terms of **rules** that are specified in a **Snakefile**\"*.\n",
    "\n",
    "Let's start discussing these three concepts:\n",
    " * a *Snakefile* is the equivalent of a Makefile in GNU Make. It defines the tasks and the dependencies between the tasks. It is defined using a language that extends Python: if a code snippet is valid in Python it will also be valid in  a Snakefile, but not viceversa.\n",
    " * a *rule* define a single task, possibly executed multiple times on different outputs or with different configurations, exposing to Snakemake the files it depends on, the outputs it generates and the computing resources it needs to run.\n",
    " * the *workflow* is the ensemble of tasks defined by Snakemake based on the Snakefile, ensuring that the dependencies of each task are satisfied before the task is executed and refreshing all the outputs depending on an updated dependency.\n",
    "\n",
    "Let's see our first example of a Snakefile defining a workflow with a single rule.\n",
    "\n",
    "We will name the Snakefile `Snakefile.helloworld` and the single rule in it `greetings`. \n",
    "The task defined by the rule `greetings` will simply be printing out some text relyong on the shell command `echo`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea32b4-2a2a-4cae-b52e-bfb76ffe494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.helloworld\n",
    "\n",
    "rule greetings:\n",
    "  shell:\n",
    "    \"echo 'Hello, snakemakers'\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e51bb-87b1-493f-8bf3-60cdbb9095a4",
   "metadata": {},
   "source": [
    "Note that `rule` and `shell` are keywords added to the Snakemake language on top of Python.\n",
    "The line `rule greetings:` initialize a new code block defining the dependencies, the output, the requested resources, the configuration and the task of the `greetings` rule.\n",
    "\n",
    "The line `shell:` indicates we are defining the task using shell (typically `bash`) language.\n",
    "\n",
    "That's it. You can now execute this workflow by calling snakemake and specifying the name of the Snakefile and maximal number of cores Snakemake can use.\n",
    "\n",
    "In the following examples we will always run on a single process (`-j 1`), but here we will ask Snakemake to use all the available cores (`-j all`).\n",
    "This can be dangerous in some environment. On SWAN for example, running on a [Kubernetes](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/) cluster, Snakemake finds the number of cores of the underlying virtual machine, rather than the total amount of CPUs allocated by Kubernetes to our activity. \n",
    "In this case, the Linux kernel would [scale down the CPU usage](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run) to the maximum allowed and the larger number of threads would only result in larger overhead.\n",
    "\n",
    "So the bottom-line is you may use `-j all` to run Snakemake on your own resources, but as the environment you are running on is somewhat shared, you should rather switch on your brain and fix the number of parallel tasks to an optimimum for your specific workflow and environment. \n",
    "\n",
    "> **Note on parallel execution.** Snakemake does not change anything of how the single task is executed. Don't expect it to automagically parallelize your code! However it can parallelize the execution of indepedendent tasks whose dependencies are already satisfied on multiple processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82163630-15e2-4ce0-9f90-8ec3f7b6d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j all -s Snakefile.helloworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add10ce-0e5a-488a-b779-236fe1d2a2e7",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "In the example above, the string we printed out with `echo` gets embedded in the Snakemake output, defining how tasks are queued and monitoring the status of the workflow execution. This might be extremely suboptimal when multiple tasks are executed in parallel, or workflow of thousands of tasks are run.\n",
    "\n",
    "Snakemake provides a `log:` configuration key that can be used to identify the log file. \n",
    "Log files are treated by Snakemake in a very similar way to output files, but they are not deleted upon failure of the task as they may be useful to identify the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bf6e8-729d-4c9b-8b84-4272891cd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.helloworld\n",
    "\n",
    "rule greetings:\n",
    "  log: \"greetings.log\"\n",
    "  shell:\n",
    "    \"echo 'Hello, snakemakers' > {log}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd1dee-5bb1-40fb-b6b6-ca7da55b9f56",
   "metadata": {},
   "source": [
    "\n",
    "Let's execute it as above, and let's print the content of the `greetings.log` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8b595-b1e2-43be-9a61-154846e70533",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j 1 -s Snakefile.helloworld\n",
    "!cat greetings.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6b4ed-9436-4f1f-8bd3-aec9bd3c60e0",
   "metadata": {},
   "source": [
    "Note how we are  using elements of the rule configuration to define the task. The `{log}` string will be replaced by the value of the `log` variable as defined above.\n",
    "\n",
    "It may be a good idea to *name* the log files to improve the readability of the rule. \n",
    "Naming also enables defining multiple objects for the same categories, for example one may define different logs to redirect the stdout and stderr streams, or different levels of logs  (though this might be exquisitely application dependent!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae72e9-c549-4dcf-beab-ef37ab1cdb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.helloworld\n",
    "\n",
    "rule greetings:\n",
    "  log: \n",
    "    echo_output=\"greetings.log\",\n",
    "    some_other_log=\"other.log\"\n",
    "\n",
    "  shell:\n",
    "    \"echo 'Hello, snakemakers' > {log.echo_output}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0d725-16f8-4393-9bcc-ef008b537056",
   "metadata": {},
   "source": [
    "Let's execute  this hello world one more time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f87a56-9320-4440-8cc4-ccfe78f503ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j 1 -s Snakefile.helloworld\n",
    "!cat greetings.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484b811-5ecd-4163-9cc5-809d4ac874ea",
   "metadata": {},
   "source": [
    "### Hello world, but in Python\n",
    "\n",
    "Using the shell to define tasks is most common as usually tasks are defined by full-fledged third-party applications.\n",
    "For smaller tasks, however, it may be more practical to write directly in Python.\n",
    "\n",
    "This can be achived by using the  `run:` directive instead of `shell:`.\n",
    "In Python, you will be able to access directly the `log` object (and similarly the `input`, `output` and `wildcards` we will introduce later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ae467-afe4-407d-b9e8-58db1fb2057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.helloworld\n",
    "\n",
    "rule greetings:\n",
    "    log: \n",
    "        echo_output=\"greetings.log\",\n",
    "    run:\n",
    "        print(\"Hello! This is python!\", file=open(log.echo_output, \"w\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c29b45-e552-45df-bc54-717d8e8896aa",
   "metadata": {},
   "source": [
    "Let's execute the workflow with the usual command, and let's print once more the content of `greetings.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a54af-4edc-4417-919d-86a9ba7d1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j 1 -s Snakefile.helloworld\n",
    "!cat greetings.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36546a-45dc-4427-b40b-462f17bedf45",
   "metadata": {},
   "source": [
    "## Data analysis is better with data!\n",
    "\n",
    "Let's move away from the Hello World example discussed above by starting playing with some data.\n",
    "We will start with the CERN Open Data dataset obtained from the SingleMu trigger line of the CMS experiment in 2011 and processed to contain muon pairs (DiMuons, in jargon).\n",
    "\n",
    "All details on the chosen dataset are available at the link [opendata.cern.ch/record/5202](https://opendata.cern.ch/record/5202).\n",
    "\n",
    "Let's begin building our analysis workflow by obtaining a local copy of the data.\n",
    "This will be our first task.\n",
    "\n",
    "We will use the shell command `wget` to retrieve the CSV files from the OpenData portal.\n",
    "\n",
    "> **Exercise.** You may try to reimplement this task using Python. You may use the [`requests`](https://requests.readthedocs.io/en/latest/) or [`httpx`](https://www.python-httpx.org/) libraries, or you may rely directly on the [ability of pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) to retrieve remote datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d1175-7553-4ced-bbb8-ad24409a61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.dimuon\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon.csv\"\n",
    "      \n",
    "    shell:\n",
    "        \"wget https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f516fcb-5fa9-44d4-b586-a045f41823f5",
   "metadata": {},
   "source": [
    "Let's execute a single-task workflow for the last time! Fire!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc2ba7-bf46-4a88-9dfc-e99e8526a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j 1 -s Snakefile.dimuon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4226cb6-e3a6-4a9b-ab1c-23fcd742a6b3",
   "metadata": {},
   "source": [
    "### Counting the number of lines in the CSV file\n",
    "Let's add a second step to our workflow with an additional rule.\n",
    "Let's say we want to count the number of lines in the dowloaded CSV file as a proxy of the number of dimuons (plus some header lines).\n",
    "\n",
    "We will use the [`wc` command](https://it.wikipedia.org/wiki/Wc_(Unix))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2d30a-b9c4-495d-b31e-36b8e0167e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.dimuon\n",
    "\n",
    "rule count_lines:\n",
    "    input:\n",
    "      csv_file=\"dimuon.csv\"\n",
    "      \n",
    "    shell:\n",
    "        \"cat {input.csv_file} | wc\"\n",
    "\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon.csv\"\n",
    "      \n",
    "    shell:\n",
    "        \"wget https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10881254-5761-4585-8449-9cccef567717",
   "metadata": {},
   "source": [
    "Let's draw the resulting DAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5404d5d-1852-494a-a044-dc526c9d9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.dimuon --forceall --dag | dot -Tpng > dag.png\n",
    "Image.open(\"dag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96793b8d-31b2-4bcd-9877-55d5f8600908",
   "metadata": {},
   "source": [
    "And let's execute it! It will download the CSV file from the remote location and it will count its lines by running `wc`.\n",
    "\n",
    "Note that before downloading the file, it checks whether it is already available locally. \n",
    "\n",
    "If the file is available, and no `--forceall` or `-F` flags are applied, then it will not download the dataset once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de16c3f-19a6-4bf4-9d0a-e4fbca5002c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j 1 -s Snakefile.dimuon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e74f7-ee52-4cb9-bb6f-c61234613350",
   "metadata": {},
   "source": [
    "### Adding a second dataset\n",
    "\n",
    "Let's add a second dataset to make our DAG more interesting. \n",
    "\n",
    "We will use another DiMuon dataset from CMS, but this time we will take the 2010 dataset.\n",
    "You may get additional details on this dataset at  the link [opendata.cern.ch/record/303](https://opendata.cern.ch/record/303/).\n",
    "\n",
    "The simplest thing we can do is to define two separate rules for downloading the 2010 and 2011 data.\n",
    "They will create local files with the datataking year in the filename that can be used by the downstream steps to identify which rule has to be invoked to (re-)generate each local copy of the dataset.\n",
    "\n",
    "We will introduce also the [`expand` keyword](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#the-expand-function) that enables defining a list of items differing for some token only. In this case the year of datataking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18849ba2-2012-4b54-b13d-80f16b6a8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.dimuon\n",
    "\n",
    "rule count_lines:\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=[2010, 2011])\n",
    "      \n",
    "    shell:\n",
    "        \"cat {input.csv_file} | wc\"\n",
    "\n",
    "\n",
    "rule get_file_2011:\n",
    "    output:\n",
    "      csv_file=\"dimuon2011.csv\"\n",
    "      \n",
    "    shell:\n",
    "        \"wget https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv -qO {output.csv_file}\"\n",
    "\n",
    "\n",
    "rule get_file_2010:\n",
    "    output:\n",
    "      csv_file=\"dimuon2010.csv\"\n",
    "      \n",
    "    shell:\n",
    "        \"wget https://opendata.cern.ch/record/303/files/dimuon.csv -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e062ca-afcc-41e0-8939-62c015018a9f",
   "metadata": {},
   "source": [
    "Let's show the DAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44b609-ec28-442a-bccb-296d2509c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.dimuon --forceall --dag | dot -Tpng > dag.png\n",
    "Image.open(\"dag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73a507-d1a2-4028-9fa3-eeb7e326afab",
   "metadata": {},
   "source": [
    "So this is ok and it works, but it is inelegant in too many ways to be the solution we want to adopt.\n",
    "\n",
    "Let's list some bad ideas:\n",
    " * we have separate rules performing basically the same task: *downloading a CSV dataset*. If then we add another per year of datataking the number of copy-pasted rules would explode making it tedious to maintain the workflow. We would better have one rule with different arguments and some mechanism to map a data-taking year to a remote  URL from which to pick the dataset.\n",
    " * we are manually typing at least twice the years of data-taking. If we are adding one dataset, we have to manually  modify the rules, which is not nice...\n",
    "\n",
    "Fortunately we are writing in Python inside our Snakefile and we can do pretty much everything including defining a mapping of the datataking years towards the URL and then use that mapping throughout the rules. \n",
    "\n",
    "We will create a new DATASETS mapping (a `dict`) associating the year (as a string!) to the URL.\n",
    "\n",
    "```python\n",
    "DATASETS = {\n",
    "    \"2010\": \"https://opendata.cern.ch/record/303/files/dimuon.csv\",\n",
    "    \"2011\": \"https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv\",\n",
    "}\n",
    "```\n",
    "\n",
    "Then we need to read this mapping from the upstream and downstream rules. \n",
    "\n",
    "Let's start with the `count_lines` function. Everything is rather simple here, we just have to replace\n",
    "```python\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=[2010, 2011])\n",
    "```\n",
    "with\n",
    "```python\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=DATASETS.keys())\n",
    "```\n",
    "\n",
    "\n",
    "For resolving the mapping in the `get_file` rule, instead, we need an additional trick.\n",
    "\n",
    "The rule will depend on a parameter, the URL, that can be inferred based on the name of the pulled `output` file. \n",
    "When some rule will request to generate the file `dimuon2011.csv`, Snakemake must define the parameter URL as obtained from the mapping DATASETS where the key is `\"2011\"`. \n",
    "\n",
    "Do define this mechanism we need three new concepts:\n",
    " * **wildcards** ([link to the docs](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#wildcards));\n",
    " * **parameters** ([link to the docs](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#non-file-parameters-for-rules));\n",
    " * **input functions** ([link to the docs](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#input-functions)).\n",
    "\n",
    "\n",
    "#### Wildcards\n",
    "Wildcards are used to generalize rules extracting tokens from the paths of the outputs to infer parameters and inputs of a given rule. \n",
    "\n",
    "In our example, we will define the rule `get_file` to generate an output named `dimuon{year}.csv`. \n",
    "`year` is the wildcard. When a downstream rule will try request to (re-)generate `dimuon2011.csv`, the wildcard `year` will get assigned the value `\"2011\"`.\n",
    "\n",
    "> **Pro Tip.** If you speak regex, be aware it is good practice to specify what kind of token you expect in a wildcard. For example, in this way we know that the datataking year is composed of exactly for digits, so we could specify that the output filename of our rule is `dimuon{year,[0-9]{4}}.csv` or  `dimuon{year,\\d{4}}.csv`. If we accept our code will break compatibility for data taking periods in 75 years (which I do personally consider acceptable), we can force the first two digits to be `\"20\"` by writing `dimuon{year,20\\d{2}}.csv`.\n",
    "> **Regular expression** are powerful to constraint the tokens but they are considered as black magic by many people, so beware of the readability loss if you decide to adopt them.\n",
    "\n",
    "#### Parameters (`params`)\n",
    "Parameters are non-file inputs to the rule. A parameter can be built on top of a wildcard and used to define the task (while wildcards alone cannot be used as inputs).\n",
    "In our case, we will define a parameter named `url` obtained from the wildcard `year` resolving the mapping `DATASETS`.\n",
    "\n",
    "#### Input functions \n",
    "The resolution of the mapping requires some logic beyond string formatting. We can inject such logic in a function taking as an input a dictionary of wildcards and returning a string and using that function a \"parameter\". \n",
    "\n",
    "Lambda functions are particularly well suited for this kind of task!\n",
    "\n",
    "Wrapping all together, we will have our new `get_file` rule defined as:\n",
    "```python\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon{year}.csv\"   ## {year} here is a wildcard!\n",
    "\n",
    "    params:  ## params define non-file inputs\n",
    "        url=lambda wildcards: DATASETS[wildcards['year']]   \n",
    "        ## url is obtained with a function, resolving DATASETS mapping.\n",
    "\n",
    "    shell:\n",
    "        \"wget {params.url} -qO {output.csv_file}\"\n",
    "        ##     ^^^^^^^^^^ the parameter url is then used to define the task \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a65289-a5e3-4f5b-889f-50f3d0865db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.dimuon\n",
    "\n",
    "DATASETS = {\n",
    "    \"2010\": \"https://opendata.cern.ch/record/303/files/dimuon.csv\",\n",
    "    \"2011\": \"https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv\",\n",
    "}\n",
    "\n",
    "def resolve_dataset(wildcards):\n",
    "    return DATASETS[wildcards['year']]\n",
    "\n",
    "\n",
    "rule count_lines:\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=DATASETS.keys())\n",
    "      \n",
    "    shell:\n",
    "        \"cat {input.csv_file} | wc\"\n",
    "\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon{year}.csv\"\n",
    "\n",
    "    params:\n",
    "        url=resolve_dataset\n",
    "        ## Or alternatively, \n",
    "        # url=lambda w: DATASETS[w['year']]\n",
    "\n",
    "    shell:\n",
    "        \"wget {params.url} -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea138c-f207-4679-913c-c286839c774d",
   "metadata": {},
   "source": [
    "Let's display the DAG. Now there is only a single `get_file` task, but it gets executed twice, once with wildcard `year = \"2010\"` and once with `year = \"2011\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8d617-3ec1-4c8b-983d-8ce605cd3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.dimuon --forceall --dag | dot -Tpng > dag.png\n",
    "Image.open(\"dag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e9344-0f7d-4915-9d2f-d4f97a24e44d",
   "metadata": {},
   "source": [
    "Let's fire it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880920f3-39e2-4e08-ac68-0f08b9eb45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j 1 -s Snakefile.dimuon --forceall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775570c-9f3d-4a35-a793-5375b190952b",
   "metadata": {},
   "source": [
    "## Writing tasks for Snakemake\n",
    "\n",
    "So far we discussed how Snakemake is used to connect existing applications to perform tasks as part of a single workflow. \n",
    "Now we will turn the perspective the other way, on the development of the tasks. \n",
    "\n",
    "We will consider three examples of tasks:\n",
    " * developing a Python applications configurable with command-line arguments and environmental variables and integrating it in snakemake via the shell\n",
    " * developing a Jupyter notebook configurable with environmental variables and plug it in the workflow;\n",
    " * developing a custom, snakemake-specific task taking inputs directly from the snakemake rule using the `snakemake` object.\n",
    "\n",
    "While we are focussing here on Python, notice the first approach is completely general, and that for the third approach there are many wrappers for many different languages to directly access, from the task, to the rule configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa245a-2724-4951-890a-ab65530a81ad",
   "metadata": {},
   "source": [
    "### Snakemake-independent tasks: using environment variables and arguments \n",
    "\n",
    "The first method we will explore to integrate custom steps in a Snakemake workflow is conceptually  the simplest one.\n",
    "\n",
    "We will write a custom application designed to be configured with its command line arguments and some environmental variables, and then we will plug it in the Snakemake workflow by calling it from the shell. \n",
    "\n",
    "This method has the advantage of producing an application that can be used independently of snakemake, it can be developed and debugged using IDEs that do not know snakemake or the other tasks and can be released to a larger audience than that interested in your specific workflow. \n",
    "\n",
    "On the other hand, as we will see, the complexity of the inputs and of the configuration might be limited by the expressivity one may reach with command line arguments and env vars.\n",
    "\n",
    "Also, integrating the two requires some gymnastics and boilerplate which you may be happy to avoid if you are writing some tasks that only make sense within your specific workflow. \n",
    "\n",
    "#### ArgumentParser\n",
    "Command-line arguments can be read using the [`sys` module](https://docs.python.org/3/library/sys.html) in Python, \n",
    "exposing them as the [`sys.argv` list](https://docs.python.org/3/library/sys.html#sys.argv). The `sys.argv` mechanism, though, is pretty rough. There is no documentation, no flags, no validation of the arguments. \n",
    "If you think `sys.argv` may be good for your application, there are good chances wrapping directly your task in Snakemake would be cleaner and more readable. \n",
    "\n",
    "A very good alternative is provided by the module [`argparse` with its object `ArgumentParser`](https://docs.python.org/3/library/argparse.html).\n",
    "\n",
    "We will not go in any detail on `argparse` as it would require another Tutorial the same lenght as this one on Snakemake, but you got a pointer.\n",
    "\n",
    "We will define a parser for the input arguments expecting one or more CSV input files. \n",
    "\n",
    "```python\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(\"drawhist.py\")\n",
    "parser.add_argument(\"input_files\", nargs='+', help=\"Input CSV files\")\n",
    "args = parser.parse_args()\n",
    "```\n",
    "\n",
    "To cut the number of examples we will be discussing here, the output filename will be passed as an environmental variable, instead.\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "OUTPUT_FILE = os.environ.get(\"OUTPUT_FILE\", \"fig.png\")\n",
    "```\n",
    "\n",
    "Then we can build some fancy logics connecting a list of CSV input files to an output figure. \n",
    "\n",
    "For example, with a greatest effort for may fantasy, I confess, we may plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbd0ce-1df5-4fe4-9842-527645afd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile drawhist.py\n",
    "\n",
    "## Standard Python Libraries\n",
    "##   ArgumentParser is for taking inputs from the command line\n",
    "from argparse import ArgumentParser\n",
    "##   os is for taking inputs from the environment variables\n",
    "import os\n",
    "\n",
    "## Numerical python ecosystem: numpy, pandas and pyplot\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Retrieve the output file name from OUTPUT_FILE\n",
    "OUTPUT_FILE = os.environ.get(\"OUTPUT_FILE\", \"fig.png\")\n",
    "\n",
    "## Retrieve the input filenames (CSV files) from the command line\n",
    "parser = ArgumentParser(\"drawhist.py\")\n",
    "parser.add_argument(\"input_files\", nargs='+', help=\"Input CSV files\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "## Loop over the CSV files, convert and store them as DataFrames \n",
    "dataframes = []\n",
    "for fname in args.input_files:\n",
    "    dataframes.append(pd.read_csv(fname)[[\"M\"]])\n",
    "\n",
    "## Concatenate all the input datasets and create the histogram\n",
    "pd.concat(dataframes).hist(\"M\", bins=np.linspace(1, 120, 121))\n",
    "\n",
    "## Store the histogram in a file\n",
    "plt.savefig(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040040b1-1fc3-49ce-b852-fca067c0c939",
   "metadata": {},
   "source": [
    "As promised, we can test our small script running it independently of Snakemake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28817fd2-6dcb-441c-a642-6026fa2f539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!OUTPUT_FILE=outputhist.png env/bin/python3 drawhist.py dimuon2011.csv dimuon2010.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c897d43-fda9-4e8e-9787-5902b783e744",
   "metadata": {},
   "source": [
    "And then integrate it in our workflow in its dedicated `make_hist` rule.\n",
    "\n",
    "```python\n",
    "rule make_hist:\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=[2010, 2011])\n",
    "\n",
    "    output:\n",
    "        figure=\"hist.png\"\n",
    "      \n",
    "    shell:\n",
    "        \"OUTPUT_FILE={output.figure} \"\n",
    "        \"python3 drawhist.py {input.csv_file}\"\n",
    "\n",
    "```\n",
    "\n",
    "Or, alternatively, using a multi-line string for the shell command, if you prefer.\n",
    "\n",
    "```python\n",
    "rule make_hist:\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=[2010, 2011])\n",
    "\n",
    "    output:\n",
    "        figure=\"hist.png\"\n",
    "      \n",
    "    shell:\n",
    "        \"\"\"\n",
    "        OUTPUT_FILE={output.figure} \\\n",
    "        python3 drawhist.py {input.csv_file}\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "Note how we are passing the values configured for the input and output to out script, passing through the command line for the input and through the `OUTPUT_FILE` environmental variable for the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b77a2-3484-422c-950b-e4613fab2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.dimuon\n",
    "\n",
    "DATASETS = {\n",
    "    \"2010\": \"https://opendata.cern.ch/record/303/files/dimuon.csv\",\n",
    "    \"2011\": \"https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "rule make_hist:\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=DATASETS.keys())\n",
    "\n",
    "    output:\n",
    "        figure=\"hist.png\"\n",
    "      \n",
    "    shell:\n",
    "        \"\"\"\n",
    "        OUTPUT_FILE={output.figure} \\\n",
    "        python3 drawhist.py {input.csv_file}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon{year}.csv\"\n",
    "\n",
    "    params:\n",
    "        url=lambda w: DATASETS[w['year']]\n",
    "      \n",
    "    shell:\n",
    "        \"wget {params.url} -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9bcd3-7678-4588-9444-27768f77b931",
   "metadata": {},
   "source": [
    "Let's plot and execute this new DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c3c85-e41a-4c4e-89c4-7bf34ad041ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.dimuon --forceall --dag | dot -Tpng > dag.png\n",
    "display(Image.open(\"dag.png\"))\n",
    "!env/bin/snakemake -j 1 -s Snakefile.dimuon --forceall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cb8f5-5753-4bbe-8045-b6b4d807c3f0",
   "metadata": {},
   "source": [
    "Now if we want to generate for every run the images in format  `png`, `pdf` and `jpg`, we can add a new rule (the default name for the *entry-point rule* is `all` and must be the first rule defined in the Snakefile.\n",
    "\n",
    "```python\n",
    "rule all:\n",
    "    input:\n",
    "        png=\"hist.png\",\n",
    "        pdf=\"hist.pdf\",\n",
    "        tiff=\"hist.jpg\",\n",
    "\n",
    "```\n",
    "\n",
    "This automatically recreates the DAG executing three times the make_histogram step. \n",
    "\n",
    "Someone may object that filling three times the histograms just for the purpose of saving it in a different format is a waste of computing resources (and it is true). But we will leave coding optimizations to the reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1034c5-b5a2-4bbb-952d-d7f5c7a41e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.dimuon\n",
    "\n",
    "DATASETS = {\n",
    "    \"2010\": \"https://opendata.cern.ch/record/303/files/dimuon.csv\",\n",
    "    \"2011\": \"https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv\",\n",
    "}\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        png=\"hist.png\",\n",
    "        pdf=\"hist.pdf\",\n",
    "        tiff=\"hist.jpg\",\n",
    "\n",
    "rule make_hist:\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=DATASETS.keys())\n",
    "\n",
    "    output:\n",
    "        figure=\"hist.{fmt}\"\n",
    "      \n",
    "    shell:\n",
    "        \"OUTPUT_FILE={output.figure} \"\n",
    "        \"python3 drawhist.py {input.csv_file}\"\n",
    "\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon{year}.csv\"\n",
    "\n",
    "    params:\n",
    "        url=lambda w: DATASETS[w['year']]\n",
    "      \n",
    "    shell:\n",
    "        \"wget {params.url} -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ad69d-aa2e-49cb-ba04-a87b8165d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.dimuon --forceall --dag | dot -Tpng > dag.png\n",
    "Image.open(\"dag.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27c337-39b1-496f-949e-3cb9a0196863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j 1 -s Snakefile.dimuon --forceall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8da5f-9d53-4ad9-adaa-f43a813bea44",
   "metadata": {},
   "source": [
    "### Notebooks and *\"graphics in the log\"* \n",
    "\n",
    "The exact same approach we used to pass variables from the workflow to the task via the environment works for [Jupyter notebooks](https://jupyter.org/). \n",
    "\n",
    "Running Jupyter notebooks as part of the workflow may be extremely effective to organize the equivalent of the `logs` of your analysis, but with graphics contents. \n",
    "You can store the generated output as HTML or Markdown files and look at them and the plots therein to visually inspect the modelling success of some task. Logs are better for software problems, but an analysis not crashing is not necessarily yielding the correct conclusions, right?\n",
    "\n",
    "> **Pro tip.** If you wish to rerun the notebooks generated and run during the execution of the workflow, you might be interested in the [`papermill` project](https://github.com/nteract/papermill) that stores in the output notebook the configuration defined by the workflow enabling further inspection.\n",
    "\n",
    "Let's create a simple notebook, named [`AnalysisNB.ipynb`](./AnalysisNB.ipynb), with one cell containing the following Python code:\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "INPUT_FILE = os.environ.get(\"INPUT_FILE\", \"dimuon2010.csv\")\n",
    "display(f\"INPUT_FILE: {INPUT_FILE}\") ## Important, this value will change when executed in the workflow. \n",
    "                                     ## It is a good idea to display it for successive debugging.\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df.hist(\"M\", bins=100)\n",
    "plt.xlabel(\"Dimuon mass [GeV/$c^2$]\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Observe how we used once again `os.environ` to read from the environment, but this time we are getting the input data (a single CSV file).\n",
    "\n",
    "And then we plug the execution of the notebook in the workflow by using the [`nbconvert` tool](https://nbconvert.readthedocs.io/en/latest/).\n",
    "\n",
    "```python\n",
    "rule make_hist_nb:\n",
    "    input:\n",
    "        csv_file=\"dimuon{year}.csv\",\n",
    "        notebook=\"AnalysisNB.ipynb\",\n",
    "\n",
    "    output:\n",
    "        report=\"report{year}.html\"\n",
    "      \n",
    "    shell:\n",
    "        \"INPUT_FILE={input.csv_file} \"\n",
    "        \"python3 -m jupyter nbconvert {input.notebook} \"\n",
    "        \"--execute \"\n",
    "        \"--to html \"\n",
    "        \"--output {output.report}  \"\n",
    "\n",
    "```\n",
    "\n",
    "In order to trigger the execution of this new rule, we also need to add `report2010.html` and `report2011.html` as inputs of the `all` rule.\n",
    "\n",
    "```python\n",
    "rule all:\n",
    "    input:\n",
    "        png=\"hist.png\",\n",
    "        pdf=\"hist.pdf\",\n",
    "        tiff=\"hist.jpg\",\n",
    "        html=expand(\"report{year}.html\", year=DATASETS.keys()),\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bf04f-0582-402f-8120-62bf23ee6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.dimuon\n",
    "\n",
    "DATASETS = {\n",
    "    \"2010\": \"https://opendata.cern.ch/record/303/files/dimuon.csv\",\n",
    "    \"2011\": \"https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv\",\n",
    "}\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        png=\"hist.png\",\n",
    "        pdf=\"hist.pdf\",\n",
    "        tiff=\"hist.jpg\",\n",
    "        html=expand(\"report{year}.html\", year=DATASETS.keys()),\n",
    "\n",
    "rule make_hist:\n",
    "    input:\n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=DATASETS.keys())\n",
    "\n",
    "    output:\n",
    "        figure=\"hist.{fmt}\"\n",
    "      \n",
    "    shell:\n",
    "        \"OUTPUT_FILE={output.figure} \"\n",
    "        \"python3 drawhist.py {input.csv_file}\"\n",
    "\n",
    "rule make_hist_nb:\n",
    "    input:\n",
    "        csv_file=\"dimuon{year}.csv\",\n",
    "        notebook=\"AnalysisNB.ipynb\",\n",
    "\n",
    "    output:\n",
    "        report=\"report{year}.html\"\n",
    "      \n",
    "    shell:\n",
    "        \"INPUT_FILE={input.csv_file} \"\n",
    "        \"python3 -m jupyter nbconvert {input.notebook} \"\n",
    "        \"--execute \"\n",
    "        \"--to html \"\n",
    "        \"--output {output.report}  \"\n",
    "\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon{year}.csv\"\n",
    "\n",
    "    params:\n",
    "        url=lambda w: DATASETS[w['year']]\n",
    "      \n",
    "    shell:\n",
    "        \"wget {params.url} -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93da250-58f2-429d-b819-4dbddf04f2d5",
   "metadata": {},
   "source": [
    "You already know the rest, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda266f-af39-4f85-a416-d076d08909ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.dimuon --forceall --dag | dot -Tpng > dag.png\n",
    "display(Image.open(\"dag.png\"))\n",
    "!env/bin/snakemake -j 1 -s Snakefile.dimuon --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcefec6-da53-490f-b9f7-574c65fdab48",
   "metadata": {},
   "source": [
    "### Snakemake-dedicated scripts: the snakemake object\n",
    "\n",
    "If the code you are developing is actually part of the workflow itself and makes no or little sense as an independent software, it is a good idea to define it as a [Snakemake script](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#python).\n",
    "\n",
    "It is possible to refer to wildcards and params in the script path, *e.g.* by specifying `\"scripts/{params.scriptname}.py\"` or `\"scripts/{wildcards.scriptname}.py\"`.\n",
    "\n",
    "Inside the script, you have access to an object snakemake that provides access to the same objects that are available in the run and shell directives: \n",
    " * `input`\n",
    " * `output`\n",
    " * `params`\n",
    " * `wildcards`\n",
    " * `log`\n",
    " * `threads`\n",
    " * `resources`\n",
    " * `config`\n",
    "\n",
    "For example, one can use `snakemake.input['csv_file]` to access the input object named `csv_file`.\n",
    "\n",
    "\n",
    "Let's create a simple script that merge the 2010 and 2011 datasets and then split the dataset in lines applying pandas queries obtained from the workflow itself. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e64134-6962-4a67-a5a3-c4da840b56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile merge_and_split.py\n",
    "\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "## Space-separated input CSV files\n",
    "INPUT_FILES = snakemake.input['csv_file']\n",
    "\n",
    "## Output CSV file name\n",
    "OUTPUT_FILE = snakemake.output[\"csv_file\"]\n",
    "\n",
    "## Optional selection string\n",
    "SELECTION = snakemake.params[\"selection\"]\n",
    "\n",
    "## Read and concatenate the CSV files\n",
    "df = pd.concat([pd.read_csv(f)[[\"M\"]] for f in INPUT_FILES])\n",
    "\n",
    "## Optionally, apply selection\n",
    "if SELECTION is not None:\n",
    "    df = df.query(SELECTION)\n",
    "\n",
    "## Output CSV file\n",
    "df.to_csv(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72708f-7dbd-4110-9920-86133dce52f3",
   "metadata": {},
   "source": [
    "Then we will add to our `Snakefile` a mapping for the `SELECTIONS` defining various bins of the invariant mass of the dimuons\n",
    "```python\n",
    "SELECTIONS = {\n",
    "    \"Jpsi\": \"M > 3.0 and M < 3.2\",\n",
    "    \"Upsilon\": \"M > 9 and M < 11\",\n",
    "    \"Z\": \"M > 80 and M < 100\",\n",
    "}\n",
    "```\n",
    "\n",
    "The rule will now use the `script:` directive instead of `shell`, but the rest is pretty standard:\n",
    "```python\n",
    "rule select_bin:\n",
    "    input: \n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=DATASETS.keys()),\n",
    "        script=\"merge_and_split.py\",\n",
    "    \n",
    "    output:\n",
    "        csv_file=temporary(\"{bin}.csv\")\n",
    "\n",
    "    params:\n",
    "        selection=lambda w: SELECTIONS[w['bin']]\n",
    "\n",
    "    script: \"merge_and_split.py\"\n",
    "```\n",
    "\n",
    "All together, one gets the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8f1dc-ba05-473b-a289-145bb39114a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.binned_dimuon\n",
    "\n",
    "DATASETS = {\n",
    "    \"2010\": \"https://opendata.cern.ch/record/303/files/dimuon.csv\",\n",
    "    \"2011\": \"https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv\",\n",
    "}\n",
    "\n",
    "SELECTIONS = {\n",
    "    \"Jpsi\": \"M > 3.0 and M < 3.2\",\n",
    "    \"Upsilon\": \"M > 9 and M < 11\",\n",
    "    \"Z\": \"M > 80 and M < 100\",\n",
    "}\n",
    "\n",
    "rule all:\n",
    "    input: expand(\"report-{bin}.html\", bin=SELECTIONS.keys())\n",
    "\n",
    "rule make_hist_nb:\n",
    "    input:\n",
    "        csv_file=\"{bin}.csv\",\n",
    "        notebook=\"AnalysisNB.ipynb\",\n",
    "\n",
    "    output:\n",
    "        report=\"report-{bin}.html\"\n",
    "      \n",
    "    shell:\n",
    "        \"INPUT_FILE={input.csv_file} \"\n",
    "        \"python3 -m jupyter nbconvert {input.notebook} \"\n",
    "        \"--execute \"\n",
    "        \"--to html \"\n",
    "        \"--output {output.report}  \"\n",
    "\n",
    "\n",
    "rule select_bin:\n",
    "    input: \n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=DATASETS.keys()),\n",
    "        script=\"merge_and_split.py\",\n",
    "    \n",
    "    output:\n",
    "        csv_file=temporary(\"{bin}.csv\")\n",
    "\n",
    "    params:\n",
    "        selection=lambda w: SELECTIONS[w['bin']]\n",
    "\n",
    "    script: \"merge_and_split.py\"\n",
    "\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon{year}.csv\"\n",
    "\n",
    "    params:\n",
    "        url=lambda w: DATASETS[w['year']]\n",
    "      \n",
    "    shell:\n",
    "        \"wget {params.url} -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e5260-b8d5-4c8a-9d5b-371cf7494fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.binned_dimuon --forceall --dag | dot -Tpng > dag.png\n",
    "Image.open(\"dag.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc4711-f7f5-4e6f-9bef-33f3fd900db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -j 1 -s Snakefile.binned_dimuon --forceall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449492a0-5a65-499f-b594-618ed3a677b0",
   "metadata": {},
   "source": [
    "## Advanced topics: configuration\n",
    "\n",
    "We discussed how to define complicated workflows that can be used to parallelize a complex data analysis and focus the development on some specific task while remaining connected to the whole workflow, but to make the code **reusable*** we need one further step: **we need to separate the definition of the workflow from its configuration**.\n",
    "\n",
    "Actually, there is more than one configuration that we can think of:\n",
    " * a configuration of the workflow\n",
    " * a configuration of the computing environment.\n",
    "   \n",
    "The two should remain separate, as one may want to run the same workflow on different hardware or reuse the configuration for a specific hardware to run a slightly different workload.\n",
    "\n",
    "Both configuration can be defined in **YAML files** (or JSON if you have masochist traits in your personality).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e1168-545f-4baa-9d1d-416e385607b5",
   "metadata": {},
   "source": [
    "### Workflow configuration\n",
    "\n",
    "In the example we discussed above, it makes sense to separate the definition of the workflow from the mappings of datasets and selections. These are workflow configurations and should remain separate from the list of rules and inter-dependencies. \n",
    "\n",
    "We will write then in a separate YAML file, named **`config.binned_dimuon.yaml`** as it follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46a662-1821-4bd9-ae4f-68f9fa308c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.binned_dimuon.yaml\n",
    "\n",
    "datasets:\n",
    "    \"2010\": \"https://opendata.cern.ch/record/303/files/dimuon.csv\"\n",
    "    \"2011\": \"https://opendata.cern.ch/record/5202/files/Dimuon_SingleMu.csv\"\n",
    "\n",
    "selections:\n",
    "    Jpsi:    \"M > 3.0 and M < 3.2\"\n",
    "    Upsilon: \"M > 9 and M < 11\"\n",
    "    Z:       \"M > 80 and M < 100\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a09e63-7c07-4512-b18d-59f69da83dcb",
   "metadata": {},
   "source": [
    "Then we define the default config file in the Snakefile.\n",
    "\n",
    "You can then **extend** the default dictionary by passing another configuration file with the command line option `--configfile <some/file.yaml>`or edit single entries using the `--config` argument. [Refer to the docs](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) for instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea3fdc-0b1d-472a-8d5d-b223cb74b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Snakefile.binned_dimuon\n",
    "\n",
    "configfile: \"config.binned_dimuon.yaml\"\n",
    "\n",
    "rule all:\n",
    "    input: expand(\"report-{bin}.html\", bin=config['selections'].keys())\n",
    "\n",
    "rule make_hist_nb:\n",
    "    input:\n",
    "        csv_file=\"{bin}.csv\",\n",
    "        notebook=\"AnalysisNB.ipynb\",\n",
    "\n",
    "    log:\n",
    "        html_report=\"report-{bin}.html\"\n",
    "      \n",
    "    shell:\n",
    "        \"INPUT_FILE={input.csv_file} \"\n",
    "        \"python3 -m jupyter nbconvert {input.notebook} \"\n",
    "        \"--execute \"\n",
    "        \"--to html \"\n",
    "        \"--output {log.html_report}  \"\n",
    "\n",
    "\n",
    "rule select_bin:\n",
    "    input: \n",
    "        csv_file=expand(\"dimuon{year}.csv\", year=config['datasets'].keys()),\n",
    "        script=\"merge_and_split.py\",\n",
    "    \n",
    "    output:\n",
    "        csv_file=temporary(\"{bin}.csv\")\n",
    "\n",
    "    params:\n",
    "        selection=lambda w: config['selections'][w['bin']]\n",
    "\n",
    "    script: \"merge_and_split.py\"\n",
    "\n",
    "\n",
    "rule get_file:\n",
    "    output:\n",
    "      csv_file=\"dimuon{year}.csv\"\n",
    "\n",
    "    params:\n",
    "        url=lambda w: config['datasets'][w['year']]\n",
    "      \n",
    "    shell:\n",
    "        \"wget {params.url} -qO {output.csv_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12057ea5-e07f-440f-81df-0652c65e2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.binned_dimuon --forceall --dag | dot -Tpng > dag.png\n",
    "Image.open(\"dag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce5afc-ec22-4623-9213-b834dd0eac8e",
   "metadata": {},
   "source": [
    "Now, let's add a selection, for example for the $\\psi(2S)$ state. Let's start defining a config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa7945-35f1-484a-8538-fd7885b2a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.psi2s.yaml\n",
    "\n",
    "selections:\n",
    "    psi2S: M > 3.6 and M < 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbc600-7a44-4c1a-b47f-2da31e6ff245",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.binned_dimuon --forceall --dag --configfile config.psi2s.yaml | dot -Tpng > dag.png\n",
    "Image.open(\"dag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d404801-5e3e-4dfd-8511-d03c71c1356a",
   "metadata": {},
   "source": [
    "Very similar result can be obtained modifying the configuration from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02de9b4-9bea-4ab8-8232-abaa554cbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.binned_dimuon --forceall --dag --config selections='{\"Psi2S\": \"M > 3.6 and M < 3.8\"}' | dot -Tpng > dag.png\n",
    "Image.open(\"dag.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b985f-f127-47b0-b06f-1f1c9ceca006",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env/bin/snakemake -s Snakefile.binned_dimuon -j1 --config selections='{\"Psi2S\": \"M > 3.6 and M < 3.8\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5963c5d-7012-446b-9c06-8aca3681e646",
   "metadata": {},
   "source": [
    "### Execution configuration\n",
    "\n",
    "Let's move now to discuss the configuration of the execution. \n",
    "\n",
    "To modify the way the code is executed we need to insert some configurable layer between the Snakemake sumbission of the task to the compute backend and the actual execution.\n",
    "\n",
    "Since this feature is mainly used to customize the submission procedure to some computing cluster, this feature goes under the name of `cluster execution`. \n",
    "\n",
    "> **Note.** We are in a phase of breaking changes while Snakemake 7 is no longer maintained and Snakemake 8 changes a bit the syntax. Here we are using Snakemake 7, while trying to avoid configuration elements for which it is not clear how to migrate towards Snakemake 8.\n",
    "\n",
    "To make a simple example of how we can modify the submission mechanism without introducing batch system and other workflow managements, we will just change the priority of the submitted jobs with the [`nice` Linux command](https://en.wikipedia.org/wiki/Nice_(Unix)). \n",
    "\n",
    "The nice value indicates how a process should be *nice* to others: in practice a higher nice value means lower priority. A negative nice value (requires being administrator) means higher-than-normal priority and should not be used for computing-intense tasks.\n",
    "\n",
    "Let's write our custom submission handler with a flag to set the nice value.\n",
    "\n",
    "> **Exercise.** Try to modify the submission script to introduce a different nice value for different rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d935e56-21d5-4851-b7e9-84221aea8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my_exec\n",
    "#!env/bin/python3 \n",
    "\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from snakemake.utils import read_job_properties\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"script\", help=\"bash script to be executed\")\n",
    "parser.add_argument(\"--nice\", \"-n\", type=int, default=0, help=\"nice value\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "## Note, we are retrieving the properties of the job (such as the name of the rule,\n",
    "## the requested resources, the inputs and the outputs).\n",
    "job = read_job_properties(args.script)\n",
    "\n",
    "## Some logging for monitoring purpose.\n",
    "print (job, file=open(f\"{job['rule']}.log\", \"w\"))\n",
    "\n",
    "## The real submission with given nice value.\n",
    "os.system(f\"nice -{args.nice} /bin/bash -norc {args.script}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e95d22-cc4f-40c0-9cc4-46c032d071ca",
   "metadata": {},
   "source": [
    "And then we configure our Snakemake with a special value of `--nice` in our custom submitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac52d9-5844-4a06-83be-70814274358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x my_exec\n",
    "!env/bin/snakemake \\\n",
    "    --cluster \"./my_exec --nice 8\" \\\n",
    "    --snakefile Snakefile.binned_dimuon \\\n",
    "    --jobs 8 \\\n",
    "    --forceall "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824c2d9-4228-4a3c-804e-e952c413a4ba",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Snakemake is a powerful tool to define workflow and provides a rudimental runtime to manage parallel execution of the DAG on both local and cluster resources. \n",
    "\n",
    "More advanced workflow management tools exists but they are often less oriented to data analysis and numerical application development and rather to automation, which is subject to slightly different challenges.\n",
    "You may want to check at least:\n",
    " * MLFlow, https://mlflow.org/\n",
    " * Apache Airflow, https://airflow.apache.org/\n",
    " * NextFlow, https://www.nextflow.io/\n",
    " * KubeFlow, https://www.kubeflow.org/\n",
    "\n",
    "Some of these tools can also be used as backend for cluster execution of Snakemake-defined workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49038249-9465-45f6-80c0-2fc9a27ecb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
